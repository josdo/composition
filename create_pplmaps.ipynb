{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on images in abraham_isaac\n",
      " 45%|████████████████████                        | 5/11 [00:03<00:04,  1.22it/s]No predicted boxes for img datasets/stories/imgs/abraham_isaac/DT200612.jpg\n",
      "100%|███████████████████████████████████████████| 11/11 [00:06<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "!python create_pplmaps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose batch and model settings\n",
    "# dirs = {\n",
    "#     \"imgs\":\"datasets/stories/imgs\",\n",
    "#     \"maps\":\"datasets/stories/pplmaps\",\n",
    "#     \"preds\":\"datasets/stories/predictions\",\n",
    "# }\n",
    "\n",
    "# weights_path = './logs/peopleart_coco/efficientdet-d0_149_600.pth' # 150 epochs, last layer\n",
    "\n",
    "# # for model\n",
    "# threshold = 0.5\n",
    "# iou_threshold = 0.5\n",
    "# obj_list = ['person']\n",
    "\n",
    "# compound_coef = 0\n",
    "# use_cuda = True\n",
    "# use_float16 = False\n",
    "\n",
    "# # for pplmap\n",
    "# downscale = 1 # along each img dimension\n",
    "# map_shade = 64 # allows up to 4 boxes to overlap\n",
    "\n",
    "# # Load model\n",
    "# def load_model(weights_path, obj_list, compound_coef, use_cuda = True, use_float16 = False):\n",
    "#     cudnn.fastest = True \n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "#     model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n",
    "#                                  # TODO: replace this part with your project's anchor config\n",
    "#                                  ratios=[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)],\n",
    "#                                  scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "#     model.load_state_dict(torch.load(weights_path)) # TODO: just weight path?\n",
    "#     model.requires_grad_(False)\n",
    "#     model.eval()\n",
    "\n",
    "#     if use_cuda:\n",
    "#         model = model.cuda()\n",
    "#     if use_float16:\n",
    "#         model = model.half()\n",
    "        \n",
    "#     return model\n",
    "\n",
    "# model = load_model(weights_path, obj_list, compound_coef, use_cuda, use_float16)\n",
    "\n",
    "# # Process images \n",
    "# def format_for_prediction(img_path, compound_coef, use_cuda = True):\n",
    "#     force_input_size = None  # set None to use default size\n",
    "#     input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "#     input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "#     ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n",
    "\n",
    "#     if use_cuda:\n",
    "#         x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "#     else:\n",
    "#         x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "#     x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "#     return ori_imgs, framed_metas, x\n",
    "\n",
    "# for story in [\"adam_eve\",\"last_supper\", \"abraham_isaac\"]:\n",
    "#     path = Path(os.path.join(dirs['imgs'], story))\n",
    "#     batch_paths = [p.as_posix() for p in path.glob('*')]\n",
    "    \n",
    "#     for img_path in tqdm(batch_paths):\n",
    "#         # Pre-process\n",
    "#         ori_imgs, framed_metas, x = format_for_prediction(img_path, compound_coef, use_cuda)\n",
    "    \n",
    "#         # Predict\n",
    "#         with torch.no_grad():\n",
    "#             features, regression, classification, anchors = model(x)\n",
    "#             regressBoxes = BBoxTransform()\n",
    "#             clipBoxes = ClipBoxes()\n",
    "#             out = postprocess(x,\n",
    "#                               anchors, regression, classification,\n",
    "#                               regressBoxes, clipBoxes,\n",
    "#                               threshold, iou_threshold)\n",
    "\n",
    "#         # Post-process\n",
    "#         out = invert_affine(framed_metas, out)\n",
    "\n",
    "#         for i in range(len(ori_imgs)):\n",
    "#             img = ori_imgs[i].copy()\n",
    "#             bbox_coords = out[i]['rois'].astype(np.int)\n",
    "\n",
    "#             if not len(bbox_coords):\n",
    "#                 print(\"No predicted boxes for img %s\" % img_path)\n",
    "#                 continue\n",
    "            \n",
    "#             # Single channel pplmap\n",
    "#             mapw, maph = np.asarray(img.shape[:2]) // downscale\n",
    "#             pplmap = np.zeros((map_w, map_h))\n",
    "            \n",
    "#             for j in range(len(bbox_coords)):\n",
    "#                 (x1, y1, x2, y2) = bbox_coords[j]\n",
    "#                 print(bbox_coords[j])\n",
    "#                 # Update pplmap\n",
    "#                 mapx1, mapy1, mapx2, mapy2 = np.array((x1, y1, x2, y2)) // downscale\n",
    "#                 pplmap[mapy1:mapy2+1, mapx1:mapx2+1] += map_shade\n",
    "                \n",
    "#                 # Plot bbox predictions\n",
    "#                 cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "#                 obj = obj_list[out[i]['class_ids'][j]]\n",
    "#                 score = float(out[i]['scores'][j])\n",
    "\n",
    "#                 cv2.putText(img, '{}, {:.3f}'.format(obj, score),\n",
    "#                             (x1, y1 + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "#                             (255, 255, 0), 1)\n",
    "#                 plt.imshow(img)\n",
    "            \n",
    "#             # Save as files\n",
    "#             map_path = os.path.join(dirs['maps'], story, os.path.basename(img_path))\n",
    "#             cv2.imwrite(map_path, pplmap)\n",
    "            \n",
    "#             viz_path = os.path.join(dirs['preds'], story, os.path.basename(img_path))\n",
    "#             plt.savefig(viz_path)\n",
    "#             plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
